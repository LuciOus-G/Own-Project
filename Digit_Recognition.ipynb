{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Digit_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOmmfF2ftgiPYEZhP4M2W68",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuciOus-G/Own-Project/blob/Portfolio/Digit_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV43_GbFeEOz"
      },
      "source": [
        "**first install and import the library that we needed**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvPMZg2-HFg-"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, MaxPool2D, Flatten, Dropout, Conv2D\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import normalize"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCR5SJVXeQxI"
      },
      "source": [
        "**next we're gonna preprocessing the datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0C-o-itH5Q3"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = normalize(x_train, axis=1)\n",
        "x_test = normalize(x_test, axis=1)\n",
        "\n",
        "print (x_train[10].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voJhw6sQebU9"
      },
      "source": [
        "**and then we make an our models and compile it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lqz_GlYKidd"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense( 128, activation='relu' ))\n",
        "model.add(Dense( 64, activation='relu' ))\n",
        "model.add(Dense( 10, activation='softmax' ))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2U80tsRMafZ"
      },
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics= ['accuracy']\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WUFBQGYeh0S"
      },
      "source": [
        "and we have to train our models with our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9la8XDnHM1iL"
      },
      "source": [
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs = 100,\n",
        "    batch_size = 256,\n",
        "    shuffle = True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3yT8b5YMMEB"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR2oc1LajOMB"
      },
      "source": [
        "**okay this literally it's done, but when we just print the predict, that's gonna print the all of probability from the x_test datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnBSMONsRSxK"
      },
      "source": [
        "predict = model.predict([x_test])\n",
        "print (predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPV6Zeg4kO3B"
      },
      "source": [
        "so we're gonna use np.argmax to find the maximum of the value from prediction,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SGKQsO9hnuF"
      },
      "source": [
        "\"\"\"\n",
        "  the code is simply actualy just like this\n",
        "    #print (np.argmax(predict[10]))\n",
        "    or\n",
        "    #predict = np.argmax(predict[10])\n",
        "    #print (predict)\n",
        "\n",
        "  i just make a looping and random number for giving a different of prediction every we run the cell\n",
        "\"\"\"\n",
        "\n",
        "for ran in range(10):\n",
        "  random = np.random.randint(len(x_test))\n",
        "  print (np.argmax(predict[random]))\n",
        "  plt.imshow(x_test[random])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}